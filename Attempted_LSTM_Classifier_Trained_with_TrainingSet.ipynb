{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'reviews_Books_5.json.gz'\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions for getting data from datafile\n",
    "import gzip\n",
    "def parse(path, topN=None):\n",
    "    with gzip.open(path, 'r') as g:\n",
    "        lc = 0\n",
    "        for l in g:\n",
    "            lc += 1\n",
    "            yield eval(l)\n",
    "            if topN != None and lc == topN: break\n",
    "                \n",
    "def extractWithFeedback(data, n, minTotalFeedbacks=10):\n",
    "    reviews_w_fb = []\n",
    "    for i in parse(data):\n",
    "        if len(i['reviewText']) < 10:\n",
    "            continue\n",
    "        if i['helpful'][1] > minTotalFeedbacks:\n",
    "            reviews_w_fb.append(i)\n",
    "        if len(reviews_w_fb) == n:\n",
    "            break\n",
    "    return reviews_w_fb\n",
    "\n",
    "def getData(data, totalSample, useCache=True, minTotalFeedbacks=10, split=[0.6,0.2,0.2]):\n",
    "    import pickle\n",
    "    import os\n",
    "    if useCache and os.path.isfile(\"train_data.p\") and os.path.isfile(\"dev_data.p\") and os.path.isfile(\"test_data.p\"):\n",
    "        print(\"using cached data\")\n",
    "        with open(\"train_data.p\", \"rb\") as f:\n",
    "            train_data = pickle.load(f)\n",
    "        with open(\"dev_data.p\", \"rb\") as f:\n",
    "            dev_data = pickle.load(f)\n",
    "        with open(\"test_data.p\", \"rb\") as f:\n",
    "            test_data = pickle.load(f)\n",
    "        return train_data, dev_data, test_data\n",
    "            \n",
    "    # probably not needed, but shuffle the data just to be safe\n",
    "    samples = np.random.permutation(extractWithFeedback(data, totalSample, minTotalFeedbacks))\n",
    "    split_idx1 = int(split[0]*len(samples))\n",
    "    split_idx2 = split_idx1+int(split[1]*len(samples))\n",
    "    train_data = samples[:split_idx1]\n",
    "    dev_data = samples[split_idx1:split_idx2]\n",
    "    test_data = samples[split_idx2:]\n",
    "    \n",
    "    with open(\"train_data.p\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(\"dev_data.p\", \"wb\") as f:\n",
    "        pickle.dump(dev_data, f)\n",
    "    with open(\"test_data.p\", \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    \n",
    "    return train_data, dev_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached data\n"
     ]
    }
   ],
   "source": [
    "total_sample = 10000\n",
    "train_data, dev_data, test_data = getData(datafile, total_sample, useCache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asin': '000649885X',\n",
       " 'helpful': [6, 21],\n",
       " 'overall': 1.0,\n",
       " 'reviewText': \"After reading The Farseer Trilogy which was very promising, I decided to take another chance with the author and try The Ship of Magic series. This first book is very silly. The concept of a liveship which speaks to it's  occupants was intriguing but it loses it's appeal as the characters were  very annoying, and the magic was very unsophisticated. I tried very hard to  find some good in this novel but it falls short of anything spectacular.  Only for the extreemely imaginative and open minded. If you have your feet  planted firmly on the ground, this fantasy novel is not for you. Goodkind  fans will find this novel a hard read. The Farseer trilogy is far better.\",\n",
       " 'reviewTime': '05 6, 2000',\n",
       " 'reviewerID': 'A3SPHSI6Q9HO1G',\n",
       " 'reviewerName': 'Amazon Customer \"funnicky\"',\n",
       " 'summary': 'For the extremely imaginative only !',\n",
       " 'unixReviewTime': 957571200}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize and preprocess text\n",
    "import utils; reload(utils)\n",
    "def preprocessAll():\n",
    "    for dataset in (train_data, test_data, dev_data):\n",
    "        for data in dataset:\n",
    "            raw_text = data['reviewText']\n",
    "            sentences = sent_tokenize(raw_text)\n",
    "            final_tokens = []\n",
    "            for s in sentences:\n",
    "                final_tokens.append('<s>')\n",
    "                for w in word_tokenize(s):\n",
    "                    final_tokens.append(utils.canonicalize_word(w))\n",
    "            final_tokens.append('</s>')\n",
    "            data['procTokens'] = final_tokens\n",
    "preprocessAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save processed text\n",
    "import pickle\n",
    "with open('train_data_proc.p', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open('dev_data_proc.p', 'wb') as f:\n",
    "    pickle.dump(dev_data, f)\n",
    "with open('test_data_proc.p', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pickle\n",
    "with open('train_data_proc.p', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('dev_data_proc.p', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "with open('test_data_proc.p', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "class vocabulary:\n",
    "    START_TOKEN = \"<s>\"\n",
    "    END_TOKEN = \"</s>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    \n",
    "    def __init__(self, train_data, test_data, dev_data, size):\n",
    "        self.unigram_counts = collections.Counter(flatten([t['procTokens'] for t in train_data])\n",
    "                                                  +flatten([t['procTokens'] for t in test_data])\n",
    "                                                  +flatten([t['procTokens'] for t in dev_data]))\n",
    "        top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "        vocab = ([self.UNK_TOKEN] + [w for w,c in top_counts])\n",
    "        # Assign an id to each word, by frequency\n",
    "        self.id_to_word = dict(enumerate(vocab))\n",
    "        self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
    "        self.size = len(self.id_to_word)\n",
    "        if size is not None:\n",
    "            assert(self.size <= size)\n",
    "\n",
    "        # Store special IDs\n",
    "        self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "        self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "        self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "    def words_to_ids(self, words):\n",
    "        return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "    def sentence_to_ids(self, words):\n",
    "        return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "    def ordered_words(self):\n",
    "        \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "        return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = 10000\n",
    "vocab = vocabulary(train_data, test_data, dev_data, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate batch, and pad to same length\n",
    "def batchGenerator(dataset, batch_size, vocab, success_ratio=0.8, maxtime=None):\n",
    "    for i in xrange(0, len(dataset), batch_size):\n",
    "        batch_data = dataset[i:i+batch_size]\n",
    "        maxlength = max([len(d['procTokens']) for d in batch_data])\n",
    "        if maxtime != None and maxtime < maxlength:\n",
    "            maxlength = maxtime\n",
    "        x = []\n",
    "        y = []\n",
    "        raw = []\n",
    "        for data in batch_data:\n",
    "            tokens = data['procTokens']\n",
    "            if len(tokens) > maxlength:\n",
    "                tokens = tokens[:maxlength]\n",
    "            elif len(tokens) < maxlength:\n",
    "                tokens = tokens + ['</s>']*(maxlength-len(tokens))\n",
    "            x.append(vocab.words_to_ids(tokens))\n",
    "            helpful_ratio = data[\"helpful\"][0]*1.0/data[\"helpful\"][1]\n",
    "            raw.append(data[\"helpful\"])\n",
    "            if helpful_ratio > success_ratio:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        yield (x, y, raw)\n",
    "        \n",
    "# run like this\n",
    "#result = batchGenerator(test_data, 5, vocab, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '(', 'i', 'am', 'reviewing', 'the', 'DGDGDGDG', '<unk>', 'large', 'print', 'version', 'of', 'the', 'original', 'DGDGDGDG', 'book', 'by', '<unk>', '.', '<s>', 'illustrations', 'are', 'by', '<unk>', '<unk>', '.', '<s>', ')', 'this', 'fanciful', 'book', \"'s\", 'old-fashioned', 'style', 'and', 'content', 'almost', 'feels', 'as', 'if', 'it', 'were', 'written', 'at', 'the', 'turn', 'of', 'the', '19th', 'century', ',', 'and', 'the', 'james', \"'\", 'initial', 'misery', 'recalls', 'dickens', '.', '<s>', 'the', 'writing', \"'s\", 'rough', 'edges', 'make', 'it', 'seem', 'more', 'like', 'a', 'personal', 'story', ',', 'rather', 'than', 'the', 'product', 'of', 'some', 'anonymous', '<unk>', ',', 'the', 'beginning', 'of', 'the', 'book', '(', 'where', 'james', 'magically', 'escapes', 'from', 'his', '<unk>', ')', 'seems', 'contrived', ',', 'the', '<unk>', 'are', 'unbelievably', 'cruel', ',', 'and', 'the', 'writing', 'is', 'somehow', 'flat', '.', '<s>', 'however', ',', 'the', 'book', 'picks', 'up', 'after', 'james', 'and', 'his', '<unk>', 'insect', 'friends', 'escape', 'via', 'a', 'magic', 'peach', '.', '<s>', 'the', '<unk>', 'and', 'arguing', 'insect', 'personalities', 'are', 'reminiscent', 'of', 'those', 'in', '``', '<unk>', 'the', '<unk>', \"''\", 'and', '``', 'the', 'wizard', 'of', 'oz', '.', \"''\", '<s>', '(', 'the', '<unk>', '<unk>', 'and', 'worm', 'are', 'a', 'bit', 'like', 'emotionally', '<unk>', '<unk>', 'and', 'pessimistic', '<unk>', ';', 'the', '``', '<unk>', \"''\", 'plays', 'a', 'role', 'similar', 'to', 'the', '<unk>', '.', ')', '<s>', 'the', 'insects', \"'\", '<unk>', 'and', 'fear', 'is', 'balanced', 'by', 'james', \"'\", '<unk>', 'and', '<unk>', 'actions', 'that', 'save', 'them', 'from', 'sharks', ',', 'the', 'angry', '``', 'cloud', 'people', \"''\", '(', 'who', 'throw', '<unk>', ',', 'water', ',', 'and', '<unk>', 'paint', 'at', 'them', ')', ',', 'and', 'the', 'fearful', 'citizens', 'of', 'new', 'york', '<unk>', 'has', 'lots', 'of', 'word', 'play', '(', '``', 'oh', ',', 'just', 'look', 'at', 'the', '<unk>', 'gruesome', 'face', '!', '<s>', '``', ')', ',', 'and', 'songs', 'done', 'in', 'a', 'kind', 'of', '``', 'alice', 'of', '<unk>', \"''\", 'meets', '<unk>', 'style', ':', '``', 'i', \"'ve\", 'eaten', 'many', 'strange', 'and', '<unk>', 'dishes', 'in', 'my', 'time', ',', 'like', '<unk>', '<unk>', 'and', '<unk>', 'and', '<unk>', 'cooked', 'in', '<unk>', ',', 'and', 'mice', 'with', '<unk>', \"'re\", 'really', '<unk>', '<unk>', 'in', 'their', 'prime', '.', '<s>', '(', 'but', 'do', \"n't\", 'forget', 'to', '<unk>', 'them', 'with', 'just', 'a', '<unk>', 'of', '<unk>', '.', ')', \"''\", '<s>', 'as', 'you', 'can', 'see', ',', 'the', 'humor', '(', 'and', 'some', 'of', '<unk>', '<unk>', \"'s\", 'illustrations', 'for', 'the', 'DGDGDGDG', 'edition', ')', 'is', 'sometimes', '<unk>', ',', 'and', 'the', 'demise', 'of', 'the', '<unk>', 'is', 'not', 'like', 'that', 'pictured', 'in', 'the', 'movie', 'based', 'on', 'the', 'book', '(', 'they', 'get', 'run', 'over', 'by', 'the', 'peach', '.', ')', '<s>', 'overall', ',', 'however', ',', 'and', 'despite', 'its', 'slow', 'beginning', ',', 'the', 'book', 'is', '<unk>', 'and', 'lots', 'of', 'fun', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for r in batchGenerator(test_data[0:1], 1, vocab, 200):\n",
    "    print(vocab.ids_to_words(r[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Params:\n",
    "trained_filename = 'tf_saved/final_project_lstm_classifier'\n",
    "model_params = dict(V=V, H=100, num_layers=2)\n",
    "\n",
    "# Training parameters\n",
    "max_time = 400\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "keep_prob = 1.0\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def matmul3d(X, W):\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def makeCell(self, H, keep_prob, num_layers=1):\n",
    "        cell_layers = []\n",
    "        for i in range(num_layers):\n",
    "            cell_layers.append(tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=H, forget_bias=0.0, initializer=tf.contrib.layers.xavier_initializer(), state_is_tuple=False), output_keep_prob=keep_prob))\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cell_layers, state_is_tuple=False)\n",
    "\n",
    "    def __init__(self, V, H, num_layers=1):\n",
    "        self.V = V\n",
    "        self.H = H\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        with tf.name_scope(\"Training_Parameters\"):\n",
    "            self.learning_rate_ = tf.constant(0.1, name=\"learning_rate\")\n",
    "            self.dropout_keep_prob_ = tf.constant(1.0, name=\"dropout_keep_prob\")\n",
    "            self.max_grad_norm_ = 5.0\n",
    "\n",
    "        self.input_w_ = tf.placeholder(tf.int32, [None, None], name=\"w\")\n",
    "        self.target_y_ = tf.placeholder(tf.float32, [None], name=\"y\")\n",
    "\n",
    "        with tf.name_scope(\"batch_size\"):\n",
    "            self.batch_size_ = tf.shape(self.input_w_)[0]\n",
    "        with tf.name_scope(\"max_time\"):\n",
    "            self.max_time_ = tf.shape(self.input_w_)[1]\n",
    "            self.ns_ = tf.tile([self.max_time_], [self.batch_size_,], name=\"ns\")\n",
    "\n",
    "        embedding = tf.get_variable(\"embedding\", [self.V, self.H], dtype=tf.float32\n",
    "                                    ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_w_)\n",
    "\n",
    "        lstm_cell = self.makeCell(self.H, self.dropout_keep_prob_, self.num_layers)\n",
    "        self.initial_h_ = lstm_cell.zero_state(self.batch_size_, dtype=tf.float32)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_cell, inputs, initial_state=self.initial_h_)\n",
    "\n",
    "        self.lstm_output = outputs\n",
    "        self.final_state = state\n",
    "        \n",
    "        # build a 2-layer NN on top of final state to perform text classification\n",
    "        with tf.variable_scope(\"classification_hidden_layer\"):\n",
    "            self.w_c_h = tf.get_variable(\"w_c_h\", shape=[H*2*self.num_layers, H], dtype=tf.float32,\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.b_c_h = tf.get_variable(\"b_c_h\", dtype=tf.float32, \n",
    "                         initializer=tf.zeros_initializer([H]))\n",
    "            self.c_h = tf.tanh(tf.matmul(self.final_state, self.w_c_h) + self.b_c_h, name=\"c_h\")\n",
    "            \n",
    "        with tf.variable_scope(\"output_layer\"):\n",
    "            self.w_out = tf.get_variable(\"W_out\", shape=[H, 1], dtype=tf.float32, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.b_out = tf.get_variable(\"b_out\", dtype=tf.float32, \n",
    "                           initializer=tf.zeros_initializer([1]))\n",
    "            self.logits_ = tf.add(tf.matmul(self.c_h, self.w_out), self.b_out, name=\"logits\")\n",
    "            \n",
    "        with tf.name_scope(\"loss_function\"):\n",
    "            self.point_loss_ = tf.nn.sigmoid_cross_entropy_with_logits(tf.squeeze(self.logits_), self.target_y_)\n",
    "            #self.loss_ = tf.reduce_sum(self.point_loss_)\n",
    "            self.loss_ = tf.reduce_mean(self.point_loss_)\n",
    "            \n",
    "        # train ops\n",
    "        with tf.name_scope(\"train_ops\"):\n",
    "            #tvars = tf.trainable_variables()\n",
    "            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss_, tvars),self.max_grad_norm_)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(self.learning_rate_)\n",
    "            #self.train_step_ = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(self.learning_rate_)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate_)\n",
    "            self.train_step_ = optimizer.minimize(self.loss_)\n",
    "            \n",
    "        # prediction\n",
    "        with tf.name_scope(\"Prediction\"):\n",
    "            self.pred_proba_ = tf.sigmoid(self.logits_, name=\"pred_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try to dump out some intermediate data\n",
    "def dumpIntermediateOutputs(inputBatch):\n",
    "    def _flatten(sentences):\n",
    "        final_sent = []\n",
    "        for sent in sentences:\n",
    "            final_sent.append(\"<s>\")\n",
    "            for word in sent:\n",
    "                final_sent.append(word)\n",
    "        final_sent.append(\"</s>\")\n",
    "        return final_sent\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        \n",
    "        session.run(tf.initialize_all_variables())\n",
    "        w = []\n",
    "        for inputs in inputBatch:\n",
    "            padded_ids = vocab.words_to_ids(inputs)\n",
    "            w.append(padded_ids)\n",
    "        #print(w)\n",
    "            \n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #print(h)\n",
    "        feed_dict = { lm.input_w_:w,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0,\n",
    "               lm.target_y_: [1]*len(w)}\n",
    "                \n",
    "        pred, logits_ = session.run([lm.pred_proba_, lm.logits_], feed_dict)\n",
    "        return pred, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff354711890>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff394032110>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.49952438],\n",
       "        [ 0.50011986]], dtype=float32), array([[-0.00190239],\n",
       "        [ 0.00047945]], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumpIntermediateOutputs([\"this is a test\".split(), \"this is another test\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def score_batch(pred_probs, targets):\n",
    "    pred = [1 if p>0.5 else 0 for p in pred_probs]\n",
    "    accuracy = metrics.accuracy_score(targets, pred)\n",
    "    precision = metrics.precision_score(targets, pred)\n",
    "    recall = metrics.recall_score(targets, pred)\n",
    "    f1 = metrics.f1_score(targets, pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff3340789d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff3147cbed0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set baseline\n",
      "batch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff334712fd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff3147f0b10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.29594151751953618, 'recall': 0.19911804613297152, 'precision': 0.57605495583905786, 'accuracy': 0.53449999999999998}\n",
      "dev set baseline\n",
      "batch #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff3541e3e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff33448ea90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.29513343799058084, 'recall': 0.20280474649406688, 'precision': 0.5417867435158501, 'accuracy': 0.55100000000000005}\n",
      "test set baseline\n",
      "batch #0\n",
      "{'f1': 0.24066390041493776, 'recall': 0.15441959531416399, 'precision': 0.54511278195488722, 'accuracy': 0.54249999999999998}\n"
     ]
    }
   ],
   "source": [
    "# baseline score - no training\n",
    "def baselineScore(dataset):\n",
    "    # test 2 batches on the first 10 training set\n",
    "    bi = batchGenerator(dataset, len(dataset), vocab, 0.8, max_time)\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        \n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for i,(w,y, raw) in enumerate(bi):\n",
    "            print(\"batch #%s\"%i)\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "            feed_dict = { lm.input_w_:w,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "\n",
    "            pred_prob = session.run(lm.pred_proba_, feed_dict)\n",
    "            #print(pred_prob)\n",
    "            #print(y)\n",
    "            #print(raw)\n",
    "            print(score_batch(pred_prob, y))\n",
    "print(\"train set baseline\")\n",
    "baselineScore(train_data)\n",
    "print(\"dev set baseline\")\n",
    "baselineScore(dev_data)\n",
    "print(\"test set baseline\")\n",
    "baselineScore(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time\n",
    "    total_cost = 0.0\n",
    "    total_texts = 0\n",
    "    \n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        keep_prob = keep_prob\n",
    "        loss = lm.loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        keep_prob = 1.0\n",
    "        loss = lm.loss_\n",
    "        \n",
    "    for i, (w, y, _) in enumerate(batch_iterator):\n",
    "        feed_dict = {\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.dropout_keep_prob_: keep_prob,\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y\n",
    "        }\n",
    "        state = session.run(lm.initial_h_, feed_dict)\n",
    "        feed_dict[lm.initial_h_] = state\n",
    "        #print(state)\n",
    "        _, loss_val = session.run([train_op, loss], feed_dict)\n",
    "        \n",
    "        total_cost += loss_val\n",
    "        total_texts += len(w)\n",
    "        \n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_texts\n",
    "            avg_tps = total_texts / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d texts at %d wps, loss = %.3f\" % (i,total_texts, avg_tps, avg_cost)\n",
    "            tick_time = time.time()\n",
    "    #return total_cost / total_texts\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff2fce22810>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7ff31428fd50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "Training: total loss: 82.795\n",
      "[epoch 1] Completed in 0:06:21\n",
      "train score\n",
      "{'f1': 0.42285219649426531, 'recall': 0.33141112618724561, 'precision': 0.5839808726838015, 'accuracy': 0.55549999999999999}\n",
      "dev score\n",
      "{'f1': 0.40297498309668695, 'recall': 0.32146709816612729, 'precision': 0.53985507246376807, 'accuracy': 0.5585}\n",
      "[epoch 2] Starting epoch 2\n",
      "Training: total loss: 107.964\n",
      "[epoch 2] Completed in 0:06:16\n",
      "train score\n",
      "{'f1': 0.65928659286592872, 'recall': 1.0, 'precision': 0.49174311926605507, 'accuracy': 0.49216666666666664}\n",
      "dev score\n",
      "{'f1': 0.63384615384615384, 'recall': 1.0, 'precision': 0.46396396396396394, 'accuracy': 0.46450000000000002}\n",
      "[epoch 3] Starting epoch 3\n",
      "Training: total loss: 83.899\n",
      "[epoch 3] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 4] Starting epoch 4\n",
      "Training: total loss: 83.261\n",
      "[epoch 4] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 5] Starting epoch 5\n",
      "Training: total loss: 83.268\n",
      "[epoch 5] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 6] Starting epoch 6\n",
      "Training: total loss: 83.273\n",
      "[epoch 6] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 7] Starting epoch 7\n",
      "Training: total loss: 83.277\n",
      "[epoch 7] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 8] Starting epoch 8\n",
      "Training: total loss: 83.280\n",
      "[epoch 8] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.65891819400983453, 'recall': 1.0, 'precision': 0.49133333333333334, 'accuracy': 0.49133333333333334}\n",
      "dev score\n",
      "{'f1': 0.63341305090536393, 'recall': 1.0, 'precision': 0.46350000000000002, 'accuracy': 0.46350000000000002}\n",
      "[epoch 9] Starting epoch 9\n",
      "Training: total loss: 83.282\n",
      "[epoch 9] Completed in 0:02:45\n",
      "train score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.50866666666666671}\n",
      "dev score\n",
      "{'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.53649999999999998}\n",
      "[epoch 10] Starting epoch 10\n",
      "Training: total loss: 83.284\n",
      "[epoch 10] Completed in 0:02:45\n",
      "train score\n",
      "{'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.50866666666666671}\n",
      "dev score\n",
      "{'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.53649999999999998}\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "import time\n",
    "import utils; reload(utils)\n",
    "def runTraining(print_interval=5):\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        tf.set_random_seed(42)\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in xrange(1,num_epochs+1):\n",
    "            t0_epoch = time.time()\n",
    "            bi = batchGenerator(train_data, batch_size, vocab, 0.8, max_time)\n",
    "            print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "            cost = run_epoch(lm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "            print \"%s: total loss: %.03f\" % (\"Training\", cost)\n",
    "            print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "            \n",
    "            print(\"train score\")\n",
    "            bi = batchGenerator(train_data, len(train_data), vocab, 0.8, max_time)\n",
    "            for i,(w,y, raw) in enumerate(bi):\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "                feed_dict = { lm.input_w_:w,\n",
    "                               lm.initial_h_:h,\n",
    "                               lm.dropout_keep_prob_: 1.0}\n",
    "                pred_prob = session.run(lm.pred_proba_, feed_dict)\n",
    "                print(score_batch(pred_prob, y))\n",
    "                \n",
    "            print(\"dev score\")\n",
    "            bi = batchGenerator(dev_data, len(dev_data), vocab, 0.8, max_time)\n",
    "            for i,(w,y, raw) in enumerate(bi):\n",
    "                h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "                feed_dict = { lm.input_w_:w,\n",
    "                               lm.initial_h_:h,\n",
    "                               lm.dropout_keep_prob_: 1.0}\n",
    "                pred_prob = session.run(lm.pred_proba_, feed_dict)\n",
    "                print(score_batch(pred_prob, y))\n",
    "        # Save final model\n",
    "        saver.save(session, trained_filename)\n",
    "runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictClass(w):\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "        pred, logits = session.run([lm.pred_proba_, lm.logits_], feed_dict)\n",
    "    return pred, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79c5034d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe795224e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe798b255d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79ced7850>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79834f410>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79777cd10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe794a2a790>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe798ac35d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe798b25110>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79c439410>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79c6a7550>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe798175c90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79d5f1d50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79d5d6f90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79b967590>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe79ca7a0d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe7981fac90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe7952284d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe797d2b310>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7fe7957ad0d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n",
      "[1]\n",
      "(array([[ 0.07615406]], dtype=float32), array([[-2.49578691]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "bi = batchGenerator(dev_data[100:110], 1, vocab, 0.8, max_time)\n",
    "for (w,y,raw) in bi:\n",
    "    print(y)\n",
    "    #print(raw)\n",
    "    print(predictClass(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
