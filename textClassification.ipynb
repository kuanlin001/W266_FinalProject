{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import vocabulary\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (924077 tokens)\n",
      "Test set: 11468 sentences (237115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, _, _ = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Params:\n",
    "trained_filename = 'tf_saved/rnnlm_trained'\n",
    "model_params = dict(V=V, H=100, num_layers=1)\n",
    "\n",
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matmul3d(X, W):\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def makeCell(self, H, keep_prob, num_layers=1):\n",
    "        cell_layers = []\n",
    "        for i in range(num_layers):\n",
    "            cell_layers.append(tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=H, forget_bias=0.0, initializer=tf.contrib.layers.xavier_initializer(), state_is_tuple=True), output_keep_prob=keep_prob))\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cell_layers, state_is_tuple=True)\n",
    "    \n",
    "    def __init__(self, V, H, num_layers=1):\n",
    "        self.V = V\n",
    "        self.H = H\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        with tf.name_scope(\"Training_Parameters\"):\n",
    "            self.learning_rate_ = tf.constant(0.1, name=\"learning_rate\")\n",
    "            self.dropout_keep_prob_ = tf.constant(0.5, name=\"dropout_keep_prob\")\n",
    "            self.max_grad_norm_ = 5.0\n",
    "        \n",
    "        self.input_w_ = tf.placeholder(tf.int32, [None, None], name=\"w\")\n",
    "        self.target_y_ = tf.placeholder(tf.int32, [None, None], name=\"y\")\n",
    "        \n",
    "        with tf.name_scope(\"batch_size\"):\n",
    "            self.batch_size_ = tf.shape(self.input_w_)[0]\n",
    "        with tf.name_scope(\"max_time\"):\n",
    "            self.max_time_ = tf.shape(self.input_w_)[1]\n",
    "        self.ns_ = tf.tile([self.max_time_], [self.batch_size_,], name=\"ns\")\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [self.V, self.H], dtype=tf.float32\n",
    "                                   ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        inputs = tf.nn.dropout(tf.nn.embedding_lookup(embedding, self.input_w_), self.dropout_keep_prob_)\n",
    "        \n",
    "        lstm_cell = self.makeCell(self.H, self.dropout_keep_prob_, self.num_layers)\n",
    "        self.initial_h_ = lstm_cell.zero_state(self.batch_size_, dtype=tf.float32)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_cell, inputs, initial_state=self.initial_h_)\n",
    "        self.lstm_output = outputs\n",
    "        self.final_state = state\n",
    "        \n",
    "        self.output_w = tf.get_variable(\"output_w\", [self.H, self.V], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.output_b = tf.get_variable(\"output_b\", [self.V], dtype=tf.float32, initializer=tf.constant_initializer(value=0))\n",
    "        \n",
    "        self.logits_ = matmul3d(self.lstm_output, self.output_w)+self.output_b\n",
    "        self.loss_ = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits_, self.target_y_, name=\"full_loss\"))\n",
    "        \n",
    "        # train op\n",
    "        self.train_loss_ = tf.reduce_sum(tf.nn.sampled_softmax_loss(tf.transpose(self.output_w), self.output_b, tf.reshape(self.lstm_output, [-1, self.H]), tf.reshape(self.target_y_, [-1, 1]), num_sampled=100, num_classes=self.V, num_true=1))\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.train_loss_, tvars),self.max_grad_norm_)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate_)\n",
    "        self.train_step_ = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    def makePredictionGraph(self, num_samples):\n",
    "        self.pred_samples = tf.reshape(tf.multinomial(tf.reshape(self.logits_, [-1, self.V]), num_samples), [self.batch_size_, self.max_time_, num_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFinalState(input_list):\n",
    "    def _flatten(sentences):\n",
    "        final_sent = []\n",
    "        for sent in sentences:\n",
    "            final_sent.append(\"<s>\")\n",
    "            for word in sent:\n",
    "                final_sent.append(word)\n",
    "        final_sent.append(\"<s>\")\n",
    "        return final_sent\n",
    "        \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        results = []\n",
    "        for inputs in input_list:\n",
    "            padded_ids = vocab.words_to_ids(utils.canonicalize_words(_flatten(inputs), wordset=vocab.word_to_id))\n",
    "            w = np.reshape(padded_ids, [1,-1])\n",
    "            state = session.run(lm.initial_h_, {lm.input_w_: w[:,0:1], lm.dropout_keep_prob_: 1.0})\n",
    "            \n",
    "            for i in range(len(padded_ids)):\n",
    "                feed_dict = {\n",
    "                    lm.input_w_: w[:,i:i+1],\n",
    "                    lm.dropout_keep_prob_: 1.0\n",
    "                }\n",
    "                for j,(c,h) in enumerate(lm.initial_h_):\n",
    "                    feed_dict[c] = state[j].c\n",
    "                    feed_dict[h] = state[j].h\n",
    "                state = session.run(lm.final_state, feed_dict)\n",
    "                \n",
    "            results.append(np.hstack([state[0].h[0],state[0].c[0]]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFinalStateBatched(inputBatch):\n",
    "    def _flatten(sentences):\n",
    "        final_sent = []\n",
    "        for sent in sentences:\n",
    "            final_sent.append(\"<s>\")\n",
    "            for word in sent:\n",
    "                final_sent.append(word)\n",
    "        final_sent.append(\"<s>\")\n",
    "        return final_sent\n",
    "        \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            lm = LanguageModel(**model_params)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, trained_filename)\n",
    "        \n",
    "        w = []\n",
    "        for inputs in inputBatch:\n",
    "            padded_ids = vocab.words_to_ids(utils.canonicalize_words(_flatten(inputs),\n",
    "                                                                     wordset=vocab.word_to_id))\n",
    "            w.append(padded_ids)\n",
    "        #print(w)\n",
    "            \n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "        #print(h)\n",
    "        feed_dict = { lm.input_w_:w,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "                \n",
    "        state = session.run(lm.final_state, feed_dict)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[ -4.52896056e-04,  -6.52337298e-02,  -2.02672064e-01,\n",
       "         -1.18163507e-02,  -8.18470895e-01,  -1.08129680e+00,\n",
       "         -1.03290379e+00,  -2.69454509e-01,   8.94485414e-01,\n",
       "          9.36745286e-01,   9.05345529e-02,   3.33277941e-01,\n",
       "          3.09180766e-01,   6.19303644e-01,   1.33814716e+00,\n",
       "         -3.72797623e-02,   8.51642013e-01,   1.55151427e-01,\n",
       "         -1.06458470e-01,  -6.28919125e-01,   1.41737107e-02,\n",
       "          1.84937522e-01,   3.78194660e-01,   5.01458228e-01,\n",
       "          2.14116685e-02,  -2.14317989e+00,   7.57640973e-02,\n",
       "         -9.94137049e-01,   8.07621598e-01,   1.28478572e-01,\n",
       "         -3.94692197e-02,  -1.00476921e+00,  -2.81708050e+00,\n",
       "          9.04995024e-01,  -7.59235799e-01,  -1.01941276e+00,\n",
       "         -9.55294251e-01,   5.05580641e-02,   5.53285033e-02,\n",
       "         -2.20284760e-02,  -9.21310663e-01,  -3.19792449e-01,\n",
       "          9.79135633e-01,   9.15995300e-01,  -4.53105927e-01,\n",
       "         -3.74995857e-01,   1.71896607e-01,   1.47410774e+00,\n",
       "         -1.85710430e-01,   9.19979155e-01,  -2.69881580e-02,\n",
       "         -5.47746420e-01,   1.01368868e+00,   1.96267396e-01,\n",
       "         -1.82305261e-01,  -1.22067489e-01,   9.99020398e-01,\n",
       "          3.27484190e-01,  -9.79495883e-01,  -4.45440292e-01,\n",
       "         -2.33873308e-01,   9.59258437e-01,  -7.64105737e-01,\n",
       "          8.66299450e-01,   1.58868635e+00,   4.78910282e-02,\n",
       "          6.57996297e-01,   5.13308287e-01,   8.66474658e-02,\n",
       "          1.04014564e+00,   6.75888658e-02,  -5.51472558e-03,\n",
       "          1.24594820e+00,  -1.50203943e-01,   4.19082791e-01,\n",
       "         -6.73295498e-01,  -6.38115406e-03,  -2.31068826e+00,\n",
       "          3.60404793e-03,  -9.91281033e-01,   1.00225985e+00,\n",
       "          7.06313252e-01,  -2.09446937e-01,  -7.25469962e-02,\n",
       "          6.48677468e-01,  -9.80532095e-02,  -2.01650746e-02,\n",
       "          7.27973402e-01,   2.67833203e-01,  -4.95042242e-02,\n",
       "         -7.11663544e-01,  -2.13350236e-01,  -8.95050943e-01,\n",
       "          1.11347067e+00,   7.57880509e-02,   1.02904513e-01,\n",
       "         -1.94811177e+00,   1.76721394e-01,   9.98198092e-01,\n",
       "         -1.64764300e-01],\n",
       "       [ -4.52896056e-04,  -6.52337298e-02,  -2.02672064e-01,\n",
       "         -1.18163507e-02,  -8.18470895e-01,  -1.08129680e+00,\n",
       "         -1.03290379e+00,  -2.69454509e-01,   8.94485414e-01,\n",
       "          9.36745286e-01,   9.05345529e-02,   3.33277941e-01,\n",
       "          3.09180766e-01,   6.19303644e-01,   1.33814716e+00,\n",
       "         -3.72797623e-02,   8.51642013e-01,   1.55151427e-01,\n",
       "         -1.06458470e-01,  -6.28919125e-01,   1.41737107e-02,\n",
       "          1.84937522e-01,   3.78194660e-01,   5.01458228e-01,\n",
       "          2.14116685e-02,  -2.14317989e+00,   7.57640973e-02,\n",
       "         -9.94137049e-01,   8.07621598e-01,   1.28478572e-01,\n",
       "         -3.94692197e-02,  -1.00476921e+00,  -2.81708050e+00,\n",
       "          9.04995024e-01,  -7.59235799e-01,  -1.01941276e+00,\n",
       "         -9.55294251e-01,   5.05580641e-02,   5.53285033e-02,\n",
       "         -2.20284760e-02,  -9.21310663e-01,  -3.19792449e-01,\n",
       "          9.79135633e-01,   9.15995300e-01,  -4.53105927e-01,\n",
       "         -3.74995857e-01,   1.71896607e-01,   1.47410774e+00,\n",
       "         -1.85710430e-01,   9.19979155e-01,  -2.69881580e-02,\n",
       "         -5.47746420e-01,   1.01368868e+00,   1.96267396e-01,\n",
       "         -1.82305261e-01,  -1.22067489e-01,   9.99020398e-01,\n",
       "          3.27484190e-01,  -9.79495883e-01,  -4.45440292e-01,\n",
       "         -2.33873308e-01,   9.59258437e-01,  -7.64105737e-01,\n",
       "          8.66299450e-01,   1.58868635e+00,   4.78910282e-02,\n",
       "          6.57996297e-01,   5.13308287e-01,   8.66474658e-02,\n",
       "          1.04014564e+00,   6.75888658e-02,  -5.51472558e-03,\n",
       "          1.24594820e+00,  -1.50203943e-01,   4.19082791e-01,\n",
       "         -6.73295498e-01,  -6.38115406e-03,  -2.31068826e+00,\n",
       "          3.60404793e-03,  -9.91281033e-01,   1.00225985e+00,\n",
       "          7.06313252e-01,  -2.09446937e-01,  -7.25469962e-02,\n",
       "          6.48677468e-01,  -9.80532095e-02,  -2.01650746e-02,\n",
       "          7.27973402e-01,   2.67833203e-01,  -4.95042242e-02,\n",
       "         -7.11663544e-01,  -2.13350236e-01,  -8.95050943e-01,\n",
       "          1.11347067e+00,   7.57880509e-02,   1.02904513e-01,\n",
       "         -1.94811177e+00,   1.76721394e-01,   9.98198092e-01,\n",
       "         -1.64764300e-01]], dtype=float32), h=array([[ -3.38055557e-08,  -3.92198050e-03,  -8.85041878e-02,\n",
       "         -5.11901305e-07,  -5.31331062e-01,  -1.78009592e-04,\n",
       "         -1.03787281e-01,  -1.11973807e-02,   8.73223916e-02,\n",
       "          7.23659039e-01,   1.73485943e-03,   2.51737796e-02,\n",
       "          4.52887490e-02,   5.92202917e-02,   8.70286405e-01,\n",
       "         -2.86549348e-05,   1.98186771e-03,   1.88546758e-02,\n",
       "         -9.12301987e-03,  -3.71516533e-02,   8.98709462e-04,\n",
       "          5.86325210e-03,   2.40160786e-02,   7.56245404e-02,\n",
       "          8.74333352e-08,  -1.25866709e-03,   3.49445571e-03,\n",
       "         -2.62108142e-03,   1.31097823e-01,   8.39265194e-05,\n",
       "         -1.07294461e-02,  -7.56544769e-01,  -1.65885620e-04,\n",
       "          4.42239863e-04,  -1.61002495e-03,  -7.20275998e-01,\n",
       "         -7.64954075e-06,   3.01825795e-02,   6.32107584e-03,\n",
       "         -1.45992741e-03,  -3.49881702e-05,  -2.97284573e-01,\n",
       "          2.34479140e-02,   4.23919922e-03,  -5.09099045e-05,\n",
       "         -9.57479142e-03,   5.77159342e-04,   7.80659786e-04,\n",
       "         -1.61333401e-02,   6.70803368e-01,  -1.17567593e-04,\n",
       "         -8.69251266e-02,   6.88969135e-01,   5.36253559e-04,\n",
       "         -1.63281605e-01,  -6.30596140e-03,   1.01265414e-02,\n",
       "          1.25653460e-03,  -7.52846241e-01,  -1.33690899e-02,\n",
       "         -4.09553642e-04,   5.62749743e-01,  -7.00851008e-02,\n",
       "          7.60706067e-02,   5.69091644e-05,   5.48189320e-03,\n",
       "          1.47288581e-02,   5.98869286e-02,   1.64778717e-02,\n",
       "          7.72577405e-01,   1.48812286e-03,  -4.52772854e-03,\n",
       "          3.16129345e-03,  -5.34750931e-02,   2.28699278e-02,\n",
       "         -9.90750268e-04,  -4.27060769e-07,  -9.10793483e-01,\n",
       "          5.98577863e-05,  -6.71070814e-01,   1.48845343e-02,\n",
       "          3.66535783e-02,  -3.45166326e-02,  -1.13727932e-03,\n",
       "          3.17827961e-03,  -5.90356183e-04,  -2.90393922e-03,\n",
       "          6.82289601e-06,   3.75857167e-02,  -3.44353728e-03,\n",
       "         -1.10231653e-01,  -9.29670502e-03,  -2.72103008e-02,\n",
       "          1.27311528e-03,   1.94971659e-03,   8.23660637e-04,\n",
       "         -1.75699068e-03,   6.06912095e-03,   1.10191747e-03,\n",
       "         -3.38326767e-02],\n",
       "       [ -3.38055557e-08,  -3.92198050e-03,  -8.85041878e-02,\n",
       "         -5.11901305e-07,  -5.31331062e-01,  -1.78009592e-04,\n",
       "         -1.03787281e-01,  -1.11973807e-02,   8.73223916e-02,\n",
       "          7.23659039e-01,   1.73485943e-03,   2.51737796e-02,\n",
       "          4.52887490e-02,   5.92202917e-02,   8.70286405e-01,\n",
       "         -2.86549348e-05,   1.98186771e-03,   1.88546758e-02,\n",
       "         -9.12301987e-03,  -3.71516533e-02,   8.98709462e-04,\n",
       "          5.86325210e-03,   2.40160786e-02,   7.56245404e-02,\n",
       "          8.74333352e-08,  -1.25866709e-03,   3.49445571e-03,\n",
       "         -2.62108142e-03,   1.31097823e-01,   8.39265194e-05,\n",
       "         -1.07294461e-02,  -7.56544769e-01,  -1.65885620e-04,\n",
       "          4.42239863e-04,  -1.61002495e-03,  -7.20275998e-01,\n",
       "         -7.64954075e-06,   3.01825795e-02,   6.32107584e-03,\n",
       "         -1.45992741e-03,  -3.49881702e-05,  -2.97284573e-01,\n",
       "          2.34479140e-02,   4.23919922e-03,  -5.09099045e-05,\n",
       "         -9.57479142e-03,   5.77159342e-04,   7.80659786e-04,\n",
       "         -1.61333401e-02,   6.70803368e-01,  -1.17567593e-04,\n",
       "         -8.69251266e-02,   6.88969135e-01,   5.36253559e-04,\n",
       "         -1.63281605e-01,  -6.30596140e-03,   1.01265414e-02,\n",
       "          1.25653460e-03,  -7.52846241e-01,  -1.33690899e-02,\n",
       "         -4.09553642e-04,   5.62749743e-01,  -7.00851008e-02,\n",
       "          7.60706067e-02,   5.69091644e-05,   5.48189320e-03,\n",
       "          1.47288581e-02,   5.98869286e-02,   1.64778717e-02,\n",
       "          7.72577405e-01,   1.48812286e-03,  -4.52772854e-03,\n",
       "          3.16129345e-03,  -5.34750931e-02,   2.28699278e-02,\n",
       "         -9.90750268e-04,  -4.27060769e-07,  -9.10793483e-01,\n",
       "          5.98577863e-05,  -6.71070814e-01,   1.48845343e-02,\n",
       "          3.66535783e-02,  -3.45166326e-02,  -1.13727932e-03,\n",
       "          3.17827961e-03,  -5.90356183e-04,  -2.90393922e-03,\n",
       "          6.82289601e-06,   3.75857167e-02,  -3.44353728e-03,\n",
       "         -1.10231653e-01,  -9.29670502e-03,  -2.72103008e-02,\n",
       "          1.27311528e-03,   1.94971659e-03,   8.23660637e-04,\n",
       "         -1.75699068e-03,   6.06912095e-03,   1.10191747e-03,\n",
       "         -3.38326767e-02]], dtype=float32)),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFinalStateBatched([train_data[6]['reviewSentTokens'], train_data[6]['reviewSentTokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  8.39560954e-08,  -1.94323738e-03,  -3.79751041e-03,\n",
       "         -1.45918625e-06,  -2.95379907e-01,  -9.81683334e-05,\n",
       "         -7.59375170e-02,  -5.96834952e-03,   3.34183306e-01,\n",
       "          6.85884833e-01,   7.00229430e-04,   3.34813818e-02,\n",
       "          1.00655006e-02,   1.56047111e-02,   9.39357221e-01,\n",
       "         -5.80073647e-05,   1.32441835e-03,   3.05647845e-03,\n",
       "         -2.11497657e-02,  -2.08649077e-02,  -3.44171412e-02,\n",
       "          3.23171262e-03,   2.12429091e-02,   1.58758402e-01,\n",
       "          1.24644686e-07,  -3.24105704e-03,   9.03009786e-04,\n",
       "         -3.78764421e-03,   7.46508241e-02,   2.77384606e-05,\n",
       "          7.30262743e-03,  -7.56139696e-01,  -2.29340258e-05,\n",
       "          5.03067786e-05,  -6.63713668e-04,  -7.02029169e-01,\n",
       "         -4.70972554e-06,   1.73757728e-02,  -4.94100200e-03,\n",
       "          2.13381354e-04,  -2.02868105e-05,  -6.71020627e-01,\n",
       "          7.04710046e-03,   1.73677539e-03,  -7.73197826e-06,\n",
       "         -4.60054241e-02,  -6.01041131e-04,   4.49516607e-04,\n",
       "         -7.67355610e-04,   7.04651356e-01,  -1.68952945e-04,\n",
       "         -1.79795064e-02,   7.18471229e-01,   8.27504628e-05,\n",
       "         -8.95669237e-02,  -1.32220809e-03,   2.72868555e-02,\n",
       "          3.46675108e-04,  -7.58173943e-01,  -1.17594888e-02,\n",
       "         -6.87336433e-04,   6.77223563e-01,  -5.32516763e-02,\n",
       "          7.08919540e-02,   3.65900451e-05,   1.57459546e-02,\n",
       "          2.27374639e-02,   1.21575989e-01,   1.26733795e-01,\n",
       "          7.58454263e-01,   1.78972128e-04,   2.61137635e-03,\n",
       "          9.96940420e-04,  -7.07939565e-02,   8.51825066e-03,\n",
       "         -4.45966405e-04,  -2.94568707e-07,  -9.37595427e-01,\n",
       "          3.88672452e-05,  -7.30556369e-01,   8.99915770e-03,\n",
       "          1.59527063e-02,  -1.59419086e-02,   4.53745422e-04,\n",
       "          1.67277025e-03,  -1.72798595e-04,  -6.67806491e-02,\n",
       "          1.14477980e-05,   2.00430416e-02,  -2.39272392e-03,\n",
       "         -2.11378902e-01,  -7.53571512e-03,  -2.30471417e-03,\n",
       "          9.10294126e-04,  -4.38430253e-03,   1.26042555e-03,\n",
       "         -8.13014340e-04,   3.78159946e-03,   1.67651029e-04,\n",
       "         -2.07123589e-02,   1.88997097e-03,  -2.78217066e-02,\n",
       "         -1.71017647e-02,  -2.14290470e-02,  -6.17446005e-01,\n",
       "         -9.97845769e-01,  -1.03144550e+00,  -2.09352016e-01,\n",
       "          8.03534627e-01,   8.91384900e-01,   3.62767577e-02,\n",
       "          7.07070351e-01,   1.18194103e-01,   1.84984818e-01,\n",
       "          1.74138272e+00,  -7.84109235e-02,   8.35445285e-01,\n",
       "          5.33293448e-02,  -2.31971800e-01,  -1.92078972e+00,\n",
       "         -3.17740291e-01,   1.76413387e-01,   3.70694458e-01,\n",
       "          8.14783275e-01,   2.41077431e-02,  -2.46601009e+00,\n",
       "          4.05554622e-02,  -9.92113709e-01,   7.24382699e-01,\n",
       "          6.55956417e-02,   3.39980461e-02,  -1.00340545e+00,\n",
       "         -2.27481031e+00,   9.32284653e-01,  -1.95200300e+00,\n",
       "         -9.27862346e-01,  -8.79669487e-01,   2.49065068e-02,\n",
       "         -4.77832854e-02,   1.06374472e-02,  -9.63984847e-01,\n",
       "         -8.70139539e-01,   9.12361562e-01,   7.62166202e-01,\n",
       "         -2.85926998e-01,  -9.34397101e-01,  -3.81276965e-01,\n",
       "          1.27229404e+00,  -4.35315222e-02,   9.32750940e-01,\n",
       "         -6.01543188e-02,  -2.43805975e-01,   9.28019345e-01,\n",
       "          4.14469838e-02,  -1.07370704e-01,  -1.67425722e-02,\n",
       "          6.72513485e-01,   2.30392292e-02,  -9.91906404e-01,\n",
       "         -3.38786364e-01,  -1.06072783e+00,   9.04409349e-01,\n",
       "         -8.32514048e-01,   9.90496278e-01,   1.40967441e+00,\n",
       "          9.94455144e-02,   9.50960815e-01,   7.37123549e-01,\n",
       "          2.41992414e-01,   9.98761058e-01,   5.79069071e-02,\n",
       "          4.10655420e-03,   1.55607343e+00,  -1.86682463e-01,\n",
       "          7.27756321e-01,  -3.60078752e-01,  -1.38972737e-02,\n",
       "         -2.41808987e+00,   4.67969291e-03,  -9.95825887e-01,\n",
       "          9.76584733e-01,   4.42361355e-01,  -1.71387434e-01,\n",
       "          2.10556649e-02,   6.81608856e-01,  -6.33663088e-02,\n",
       "         -8.10476005e-01,   4.38265383e-01,   1.96639746e-01,\n",
       "         -2.18768865e-02,  -7.99263000e-01,  -2.65238494e-01,\n",
       "         -9.55197453e-01,   2.27733397e+00,  -6.48294091e-02,\n",
       "          1.11314625e-01,  -2.20533276e+00,   1.20802596e-01,\n",
       "          9.98966992e-01,  -1.75494224e-01], dtype=float32),\n",
       " array([ -2.15832641e-09,  -1.83347112e-03,  -3.27571132e-03,\n",
       "         -3.93866685e-07,  -3.06195796e-01,  -9.16202916e-05,\n",
       "         -8.22570398e-02,  -7.49640353e-03,   3.84495229e-01,\n",
       "          6.93084180e-01,  -1.08052162e-03,   2.05875747e-02,\n",
       "          1.09400041e-02,   2.06479710e-02,   9.39770937e-01,\n",
       "         -8.44947572e-05,   1.18955784e-03,   1.28660572e-03,\n",
       "         -2.55639404e-02,  -1.01888906e-02,  -3.02574337e-02,\n",
       "          3.47528094e-03,   1.74180362e-02,   1.89076722e-01,\n",
       "          2.05673331e-07,  -4.16778680e-03,   3.61741259e-04,\n",
       "         -4.44456935e-03,   9.71903056e-02,   1.94223016e-04,\n",
       "         -3.32935387e-03,  -7.56833196e-01,  -2.24231626e-05,\n",
       "          3.53147807e-05,  -5.06311655e-04,  -7.12061644e-01,\n",
       "         -3.44011823e-06,   1.51717952e-02,  -7.93836638e-03,\n",
       "         -5.65796811e-03,  -1.62825818e-05,  -6.37008667e-01,\n",
       "          7.35651841e-03,   1.45468593e-03,  -1.13638916e-05,\n",
       "         -4.26020585e-02,  -8.13730643e-04,   5.52302459e-04,\n",
       "         -9.58460208e-04,   6.79587424e-01,  -2.26286007e-04,\n",
       "         -2.46366970e-02,   7.17143357e-01,   1.92918218e-04,\n",
       "         -7.34931678e-02,  -2.20796792e-03,   3.51567417e-02,\n",
       "          2.93602381e-04,  -7.59530962e-01,  -2.09145974e-02,\n",
       "         -8.92670010e-04,   6.59130573e-01,  -7.23803341e-02,\n",
       "          6.82802796e-02,   3.50981791e-05,   1.45319691e-02,\n",
       "          2.98633724e-02,   1.28033161e-01,   1.24522313e-01,\n",
       "          7.62142658e-01,   1.89075334e-04,   3.34887509e-03,\n",
       "          7.99946138e-04,  -6.96371570e-02,   6.88840682e-03,\n",
       "         -4.70238912e-04,  -2.94067945e-07,  -8.33668232e-01,\n",
       "          2.70301825e-04,  -7.24345982e-01,   5.09824231e-03,\n",
       "          1.44463750e-02,  -9.18874890e-03,   1.04228547e-03,\n",
       "          1.23326061e-03,  -1.63644072e-04,  -8.89104530e-02,\n",
       "          1.22299061e-05,   2.76735928e-02,  -3.27715673e-03,\n",
       "         -2.12577537e-01,  -9.08413529e-03,  -2.65600276e-03,\n",
       "          7.54926645e-04,  -7.42872618e-03,   1.32240355e-03,\n",
       "         -6.35822245e-04,   1.17836404e-03,   8.46269177e-05,\n",
       "         -2.37214826e-02,  -5.10807149e-05,  -2.93387901e-02,\n",
       "         -2.09264159e-02,  -5.18639386e-03,  -5.96218169e-01,\n",
       "         -9.94433701e-01,  -1.02759588e+00,  -2.15610355e-01,\n",
       "          8.15041661e-01,   9.06793058e-01,  -6.46897256e-02,\n",
       "          7.40179420e-01,   1.34561941e-01,   2.21782506e-01,\n",
       "          1.74516797e+00,  -7.03564435e-02,   8.48997653e-01,\n",
       "          2.62747258e-02,  -2.27872774e-01,  -1.94346118e+00,\n",
       "         -3.50500762e-01,   1.70531243e-01,   3.56062442e-01,\n",
       "          8.21881354e-01,   2.71197334e-02,  -1.27301669e+00,\n",
       "          2.11290289e-02,  -9.87270713e-01,   6.96835637e-01,\n",
       "          2.48940974e-01,  -1.42419087e-02,  -1.00621414e+00,\n",
       "         -2.51370239e+00,   9.29698765e-01,  -1.94899869e+00,\n",
       "         -9.41481590e-01,  -8.86591733e-01,   2.25070845e-02,\n",
       "         -6.46400377e-02,  -2.65950382e-01,  -9.55799282e-01,\n",
       "         -8.09116364e-01,   9.25370395e-01,   7.06648469e-01,\n",
       "         -3.17109078e-01,  -9.59457278e-01,  -4.56650794e-01,\n",
       "          1.00408959e+00,  -4.74080332e-02,   8.99455786e-01,\n",
       "         -6.61307201e-02,  -2.52430558e-01,   9.20708954e-01,\n",
       "          7.07444102e-02,  -8.75130296e-02,  -2.28365064e-02,\n",
       "          6.39767289e-01,   2.02541500e-02,  -9.95105684e-01,\n",
       "         -4.12485212e-01,  -1.26293170e+00,   9.24552679e-01,\n",
       "         -8.59884381e-01,   9.85906959e-01,   1.86127067e+00,\n",
       "          8.19214433e-02,   9.57720876e-01,   7.38901377e-01,\n",
       "          2.89069593e-01,   1.00632000e+00,   5.97292818e-02,\n",
       "          5.03877737e-03,   1.95072365e+00,  -1.70004308e-01,\n",
       "          8.18078399e-01,  -3.20553750e-01,  -1.85771156e-02,\n",
       "         -1.33040786e+00,   3.21222357e-02,  -9.95504320e-01,\n",
       "          9.56886172e-01,   3.94093782e-01,  -9.91757363e-02,\n",
       "          4.15949710e-02,   6.94345951e-01,  -5.76603599e-02,\n",
       "         -7.88358390e-01,   3.49766910e-01,   2.18187809e-01,\n",
       "         -2.76132226e-02,  -8.06378245e-01,  -2.35615924e-01,\n",
       "         -9.58195567e-01,   1.85426569e+00,  -9.42818522e-02,\n",
       "          1.19180657e-01,  -1.47031641e+00,   5.30517325e-02,\n",
       "          9.98636603e-01,  -1.73871621e-01], dtype=float32)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFinalState([train_data[3]['reviewSentTokens'], train_data[4]['reviewSentTokens']])\n",
    "#train_data[0]['reviewSentTokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(train_data[500]['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = 'reviews_Books_5.json.gz'\n",
    "\n",
    "import gzip\n",
    "def parse(path, topN=None):\n",
    "    with gzip.open(path, 'r') as g:\n",
    "        lc = 0\n",
    "        for l in g:\n",
    "            lc += 1\n",
    "            yield eval(l)\n",
    "            if topN != None and lc == topN: break\n",
    "                \n",
    "def extractWithFeedback(data, n, minTotalFeedbacks=10):\n",
    "    reviews_w_fb = []\n",
    "    for i in parse(data):\n",
    "        if len(i['reviewText']) < 10:\n",
    "            continue\n",
    "        if i['helpful'][1] > minTotalFeedbacks:\n",
    "            reviews_w_fb.append(i)\n",
    "        if len(reviews_w_fb) == n:\n",
    "            break\n",
    "    return reviews_w_fb\n",
    "\n",
    "def getData(data, totalSample, useCache=True, minTotalFeedbacks=10, split=[0.6,0.2,0.2]):\n",
    "    import pickle\n",
    "    import os\n",
    "    if useCache and os.path.isfile(\"train_data.p\") and os.path.isfile(\"dev_data.p\") and os.path.isfile(\"test_data.p\"):\n",
    "        print(\"using cached data\")\n",
    "        with open(\"train_data.p\", \"rb\") as f:\n",
    "            train_data = pickle.load(f)\n",
    "        with open(\"dev_data.p\", \"rb\") as f:\n",
    "            dev_data = pickle.load(f)\n",
    "        with open(\"test_data.p\", \"rb\") as f:\n",
    "            test_data = pickle.load(f)\n",
    "        return train_data, dev_data, test_data\n",
    "            \n",
    "    # probably not needed, but shuffle the data just to be safe\n",
    "    samples = np.random.permutation(extractWithFeedback(data, totalSample, minTotalFeedbacks))\n",
    "    split_idx1 = int(split[0]*len(samples))\n",
    "    split_idx2 = split_idx1+int(split[1]*len(samples))\n",
    "    train_data = samples[:split_idx1]\n",
    "    dev_data = samples[split_idx1:split_idx2]\n",
    "    test_data = samples[split_idx2:]\n",
    "    \n",
    "    with open(\"train_data.p\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(\"dev_data.p\", \"wb\") as f:\n",
    "        pickle.dump(dev_data, f)\n",
    "    with open(\"test_data.p\", \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    \n",
    "    return train_data, dev_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached data\n"
     ]
    }
   ],
   "source": [
    "total_sample = 10000\n",
    "train_data, dev_data, test_data = getData(data, total_sample, useCache=True)\n",
    "train_targets_prob = [d[\"helpful\"][0]*1.0/d[\"helpful\"][1] for d in train_data]\n",
    "dev_targets_prob = [d[\"helpful\"][0]*1.0/d[\"helpful\"][1] for d in dev_data]\n",
    "test_targets_prob = [d[\"helpful\"][0]*1.0/d[\"helpful\"][1] for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize the datasets\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def tokenizeAll():\n",
    "    for dataset in (train_data, dev_data, test_data):\n",
    "        for data in dataset:\n",
    "            #data['reviewWordTokens'] = word_tokenize(data['reviewText'])\n",
    "            data['reviewSentences'] = sent_tokenize(data['reviewText'])\n",
    "            data['reviewSentTokens'] = [word_tokenize(s) for s in data['reviewSentences']]\n",
    "tokenizeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making train data vector...\n",
      "making test data vector...\n",
      "making dev data vector...\n"
     ]
    }
   ],
   "source": [
    "# make fix vectors from input text\n",
    "def makeFixedSizeVectors():\n",
    "    print(\"making train data vector...\")\n",
    "    vec = getFinalState([d['reviewSentTokens'] for d in train_data])\n",
    "    for i,v in enumerate(vec):\n",
    "        train_data[i]['dataVector'] = v\n",
    "    with open(\"train_data_vec.p\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "        \n",
    "    print(\"making test data vector...\")\n",
    "    vec = getFinalState([d['reviewSentTokens'] for d in test_data])\n",
    "    for i,v in enumerate(vec):\n",
    "        test_data[i]['dataVector'] = v\n",
    "    with open(\"test_data_vec.p\", \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n",
    "        \n",
    "    print(\"making dev data vector...\")\n",
    "    vec = getFinalState([d['reviewSentTokens'] for d in dev_data])\n",
    "    for i,v in enumerate(vec):\n",
    "        dev_data[i]['dataVector'] = v\n",
    "    with open(\"dev_data_vec.p\", \"wb\") as f:\n",
    "        pickle.dump(dev_data, f)\n",
    "    \n",
    "makeFixedSizeVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def trainModel():\n",
    "    #model = LogisticRegression()\n",
    "    model = GradientBoostingClassifier()\n",
    "    targets = [1 if p>0.8 else 0 for p in train_targets_prob]\n",
    "    x = [d['dataVector'] for d in train_data]\n",
    "    model.fit(x, targets)\n",
    "    return model\n",
    "\n",
    "model = trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def scoreModel(model, score_data_list, targets):\n",
    "    pred = model.predict(score_data_list)\n",
    "    accuracy = metrics.accuracy_score(targets, pred)\n",
    "    precision = metrics.precision_score(targets, pred)\n",
    "    recall = metrics.recall_score(targets, pred)\n",
    "    f1 = metrics.f1_score(targets, pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "{'f1': 0.74376098418277681, 'recall': 0.71777476255088191, 'precision': 0.77169948942377831, 'accuracy': 0.75700000000000001}\n",
      "test data\n",
      "{'f1': 0.51029252437703143, 'recall': 0.50159744408945683, 'precision': 0.51929437706725468, 'accuracy': 0.54800000000000004}\n",
      "dev data\n",
      "{'f1': 0.47766133480419193, 'recall': 0.46709816612729232, 'precision': 0.48871331828442438, 'accuracy': 0.52649999999999997}\n"
     ]
    }
   ],
   "source": [
    "print(\"train data\")\n",
    "print(scoreModel(model, [d['dataVector'] for d in train_data], [1 if p>0.8 else 0 for p in train_targets_prob]))\n",
    "print(\"test data\")\n",
    "print(scoreModel(model, [d['dataVector'] for d in test_data], [1 if p>0.8 else 0 for p in test_targets_prob]))\n",
    "print(\"dev data\")\n",
    "print(scoreModel(model, [d['dataVector'] for d in dev_data], [1 if p>0.8 else 0 for p in dev_targets_prob]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4635"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 if p>0.8 else 0 for p in dev_targets_prob])*1.0/len(dev_targets_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
